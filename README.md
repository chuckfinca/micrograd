


## Neural Network from Scratch
NOTE: This section is autogenerated by readme_ation.py, manual updates will be overwritten

### Overview
This project involves building a neural network from scratch inspired by Andrej Karpathy's video ["The spelled-out intro to neural networks and backpropagation: building micrograd"](https://www.youtube.com/watch?v=VMj-3S1tku0). The primary goal was to deepen my understanding of neural networks, backpropagation, and gradient descent by implementing these concepts manually.

### Motivation
Prior to pursuing my master's in ML/AI, my knowledge was primarily derived from several Coursera courses on machine learning and readings from Twitter. I sought a practical project to solidify this theoretical foundation. Karpathy's tutorial provided an excellent starting point due to his expertise and clear explanations.

### Technologies Used
- **Python**: Core programming language.
- **NumPy**: For numerical operations and data handling.
- **Graphviz**: For visualizing neural networks, weights, and gradients.

### Approach
1. **Learning by Doing**: Followed Karpathyâ€™s tutorial, then independently implemented and expanded upon his concepts.
2. **Extended Operations**: Added additional operations such as `tanh` to the neural network.
3. **Commenting and Documentation**: Emphasized writing clear comments to aid future understanding and reinforce learning.
4. **Custom Neuron Objects**: Implemented a `Neuron` class to simulate action potentials, though later realized `ReLU` serves this purpose in traditional neural networks.

### Challenges and Learnings
- **Labeling System**: Attempted to implement a comprehensive labeling system, which proved impractical for larger networks.
- **Understanding ReLU**: Initially misunderstood the role of `ReLU`, leading to an unnecessary `Neuron` class.
- **Practical Application**: Applied the network to a Kaggle competition task, achieving 55% accuracy (basically chance), highlighting the need for further refinement and understanding.

### Key Takeaways
- **Hands-On Experience**: Building a neural network from scratch provided deep insights into the mechanics of neural networks.
- **Importance of Iteration**: Recognized the importance of iterative learning and continuous improvement.
- **Foundation for Future Projects**: The experience laid a solid foundation for more complex machine learning projects.

### Acknowledgments
Special thanks to Andrej Karpathy for his [invaluable tutorial](https://www.youtube.com/watch?v=VMj-3S1tku0), which guided the development of this project.

<!-- END OF PROJECT DETAILS -->




## Setup and Run Instructions
NOTE: This section is autogenerated by readme_ation.py, manual updates will be overwritten

This will guide you through the process of setting up a Mamba environment and running the provided Python code to see it in action. It uses the last known working versions of Python and packages used.

### Prerequisites

Ensure you have [Mamba](https://mamba.readthedocs.io/en/latest/installation.html) installed on your system. Mamba is a fast, cross-platform package manager.

### Steps

1. **Create a Mamba Environment**
   
   Open your terminal and execute the following commands:

   ```bash
   mamba create -n myenv python=3.9.19 -y
   mamba activate myenv

2. **Install Necessary Packages**

    ```bash
    # Install each with mamba and fall back to pip if necessary
    for pkg in graphviz=2.50.0, numpy=1.26.4, readme_ation=0.1.12; do mamba install $pkg -y || pip install $pkg; done

3. **Run the Script**

    Ensure you are in your project directory and run the file you added the readme-ation code to and run:

    ```bash
    python [FILE_NAME]

    Or click 'run' in your IDE of choice.

    <!-- END SETUP AND RUN INSTRUCTIONS -->

